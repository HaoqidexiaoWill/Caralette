## 抽取式摘要模型

- 从source 中选择合适的句子作为摘要的模型，要求被选择的句子同时满足两个要求

  - 包含原文中重要的信息，并且在逻辑上保持一致
  - 具备最低的冗余性

  > 如何衡量摘要于包含原文重要信息，以及如何判断是否是最低的冗余性

- 抽取式摘要通常被视为sequence labeling task ， decoder 沿着文档挨个句子进行二分类，在每一步解码的时候综合

  - 当前句子的语义信息
  - 上一步的解码状态
  - 文档的全局语义信息判断当前的句子是否应该被选择作为摘要

> 是否是当前主流做法/未来发展方向，如果不是的话有什么弊端？

- 摘要现状：T5/Pegasus 为基础的生成式摘要模型表现超过了抽取式摘要SOTA的结果

- 2016年[文献1](https://www.aclweb.org/anthology/P16-1046.pdf)首次提出将LSTM作为decoder，每一步解码的输入是上一步句子的sentence embedding ，输出的隐藏状态经过线性层和sigmoid 进行二分类，用来预测当前句子属于摘要句子的概率，之后的LSTM/GRU 成为了主流的decoder模型，后续基于decoder 侧的改进主要集中于：

  - Hidden-state 在送入classifier 之前，应该与哪些模块/语义单元进行更有效的信息沟通

  > Char/word/sentence/para 级别 进行attention交互？

  - 如何更有效的信息沟通/交互

  > 借鉴FusionNet或者CSRAN网络 的融合方式？

![image-20201103101449213](/Users/liusongyan/Library/Application Support/typora-user-images/image-20201103101449213.png)

- 2018年[文献2](https://arxiv.org/pdf/1704.04530.pdf)提出压缩文档的时候，除了使用文档本身的内容，还可以使用一些边缘信息如下：decoder 在解码的时候，输入包含了当前句子的sentence-embedding 还包括了side -information

  - title
  - Image caption
  - Table caption

  ![image-20201103103021505](/Users/liusongyan/Library/Application Support/typora-user-images/image-20201103103021505.png)

  > 大小写/是否缩写/char 的位置句子的位置/是否存在同义词/近义词 等统计特征引进来会不会有意义

- 2018年[文献3](https://www.aclweb.org/anthology/P18-1014.pdf)指出抽取式摘要不能仅仅关注句子的重要程度，还需要关注句子中关键词的重要程度，作者认为这些关键词的往往是文档的核心实体，包含关键词的句子被选择为摘要的概率总是大于不包含关键词的句子，模型中存在两个decoder，

  - 一个是词级别的解码器每一步的输出都是词表进行筛选，代表每个词被选择的概率，
  - 另一个是句子级别的解码器，代表每个句子被选择的概率。
  - 两个解码器同步解码，在第t个时间步决定模型选择词解码器的输出还是句子解码器的输出
  - 在预测的时候，每个句子被选择为摘要的概率等于该句子被预测的概率和其所包含的所有词被预测的概率和，如果一个词被预测，那么用概率最大值作为替代

  ![image-20201103103050715](/Users/liusongyan/Library/Application Support/typora-user-images/image-20201103103050715.png)

- 除了RNN做解码器，还有一类常用的解码器（MLP+sigmoid）在解码的时候对每个句子单独进行分类，其他句子是否已经被选择为摘要对当前句子的分类没有影响。[文献4](https://arxiv.org/pdf/1611.04230.pdf)提出统合5种信息的分类器：

  $h_j = tanh(W_{st}[h_j^f,h_j^b]+b_{st})$

  $s_j = \sum_{i=1}^{j-1}{h_iP(y_i=1|h_i,s_i,d)}$ 

  $P(y_i=1|h_j,s_j,d) = \sigma(W_ch_j+h_j^TW_sd-h_j^TW_rtanh(s_j)+W_{ap}p_j^a+W_{rp}p_j^r+b)$

  - Content：$W_chj$代表第j个句子语义的影响，$h_j$是第j个句子的表示
  - Salience：$h_j^TW_sd$代表第j句在文档中重要度的影响，即信息性，d是文档的表示doc-embedding，是由所有句子的embedding 进行 average-reduction 然后过tanh 激活函数得到的
  - Novelty：$h_j^TW_rtanh(s_j)$ 代表第j句相对目前摘要冗余度的影响，$s_j$是第j步时当前摘要的表示，这是一个惩罚项，即冗余度高的句子加入摘要的概率更低
  - Absolute-position： $W_{ap}p_j^a$ 代表第j句在文档中绝对位置的影响
  - Relative-position：$W_{rp}p_j^r$ 代表第j句在文档中的相对位置的影响

- 基于上面文献的想法，[文献5](https://arxiv.org/pdf/1804.07036.pdf)提出更简单的分类公式：用RNN解码的思想迭代预测标签值

  $P(y_t=1|X,y_{1:t-1})=\sigma(W_2tanh(W_1[h_t,g_{t-1},d]+b_1)+b_2)$

  $g_t = g_{t-1}+y_t \times tanh(W_gh_t+b_g)$

  > UNILM 默默举个爪

- Encoder编码器没什么好说的，都是CNN/RNN 改编码方式，19年以后都是预训练模型编码了，

  > 需要关注的点是GCN进行结合的方式，

- 抽取式摘要任务通常被视为sequence labeling task 因此传统的抽取式摘要模型都是用decoder 逐句对待压缩文档中的每个句子进行二分类，但是这样的模型在解码的过程中，重要的信息分布在文档或者文档偏后的位置，因此[文献6](https://arxiv.org/pdf/1807.02305.pdf)提出不再将摘要任务视为序列生成任务，而是视为最有选择任务(global select task ), 具体来讲模型仍然适用RNN的方式进行循环解码，但是在每一步解码的时候，**并非对单个句子进行分类，而是计算在尚未被选择为摘要的句子上的概率分布，代表每个句子被选择为摘要的概率.**.

> 没懂..

- LOSS 改进方面，传统摘要抽取的loss 是 maximum likelihood estimation (MLE) ,最大化每个句子的gold labels 的概率，这种损失函数等价于Cross -Entropy LOSS，后续loss 的改进方向偏向RL loss（我觉得略扯）

> Ranking loss 可以继续做看样子

- 预训练模型方面比较突出的工作一个是**[BERTSUM文献7](https://arxiv.org/pdf/1903.10318.pdf)**模型，个人觉得主要是就是把BERT当encoder 然后对每个句向量做二分类

> 这里面有两个问题，BERT只是说对句子做编码，那么对于文档/段落级别的编码论文中是否有相关的工作，
>
> 超出512文档，论文中是怎么做处理的

- [文献8](https://arxiv.org/pdf/1910.14142.pdf) 和 [文献9](https://arxiv.org/pdf/2004.12393.pdf) 是BERTSUM的基础上对特征抽取部分做了较大的改进(加了图神经网络)，其中，DiscoBert将BERT的特征抽取与图卷积神经网络有效的结合在一起。但是在图网络构建的过程中，采用的是RST共指关系树的方式构造了对应的图结构，相比而言HeterSumGraph 利用异构图神经网络尝试引入更多的节点信息表示来更好的建模句子之间的关系。

![image-20201103140429559](/Users/liusongyan/Library/Application Support/typora-user-images/image-20201103140429559.png)

- [文献10](https://arxiv.org/pdf/1905.02450.pdf)思路简单，操作复杂，尝试将摘要抽取任务等价于文本匹配任务，实际上是用一种带有剪枝行为的贪心搜索。

  > 个人认为这个任务的核心工作在于candidates的构建上，因为这个构建方式决定了解码搜索空间的范围
  >
  > 因此如果能够找出更标准的抽取式候选集的方式，以及替换掉rouge的方式。应该可以达到新的上限

- [文献11](https://arxiv.org/abs/2010.07886)看不太懂，不过好像是从真实性和显著性进行建模，似乎很有道理的样子.。。

## 其他问题

- 抽取式摘要，ground truth 长度有限制嘛，固定三句话？
- 什么是 trigram blocking strategy

## 优化方向

- 文本建模方面：
  - 512句长问题
    - UNILM
  - 引入更多统计/知识特征
    - 大小写/专有名词/词性/位置信息/同义词等
    - textRank 关键词信息
  - 图神经网络结合
    - BERT+ GCN /利用外部知识构建图神经网络
- 特征交互
  - 新版Transformer  ->**Synthesizer**
  - FusionNet 字符/词/句子混合交互
- 训练任务/LOSS优化
  - Sequence labeling task 优化（global select task任务）
  - 序列生成任务
  - ranking loss/ RL loss / MLE loss

## 需要做的事情

- [ ] 
- [ ] 详细了解一下RL loss 、Ranking Loss 的收益
- [ ] 关键词指导抽取
- [ ] 序列标注给模型学
  - [ ] 实体级别主谓宾
  - [ ] ranking loss
- [ ] Baseline 跑通
- [ ] CLS 不能很好的表征这个句子
- [ ] MoverScore 只考虑2元组和L元组

### reference

1. Neural Summarization by Extracting Sentences and Words
2. Neural Extractive Summarization with Side Information
3. Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks
4. SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents
5. Learning to Extract Coherent Summary via Deep Reinforcement Learning
6. Neural Document Summarization by Jointly Learning to Score and Select Sentences
7. Fine-tune BERT for Extractive Summarization
8. Discourse-Aware Neural Extractive Text Summarization
9. Heterogeneous Graph Neural Networks for Extractive Document Summarization
10. Extractive Summarization as Text Matching
11. Compressive Summarization with Plausibility and Salience Modelin

